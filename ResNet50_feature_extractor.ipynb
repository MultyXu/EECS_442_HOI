{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCPfCpONNzjz4mGpp89/l/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpM-G7h-w9g2","executionInfo":{"status":"ok","timestamp":1699930794176,"user_tz":300,"elapsed":35604,"user":{"displayName":"Ruihan Xu (Multy)","userId":"12479151467308550886"}},"outputId":"b073e70c-69b4-49bd-87b5-04608e94c5e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","Looks that a private key is already created. If you have already push it to github, no action required.\n"," Otherwise, Please go to https://github.com/settings/ssh/new to upload the following key: \n","ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICeyklL46g42c+kOiDGQYpBIkUVa8Oott1qwDIqq8fti root@8ae8a1039f30\n","\n","Please use SSH method to clone repo.\n","/content/drive/MyDrive/Colab Notebooks/EECS_442_HOI\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# setup credential\n","!wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n","import colab_github\n","colab_github.github_auth(persistent_key=True)\n","\n","# go to directory\n","%cd /content/drive/MyDrive/Colab\\ Notebooks/EECS_442_HOI"]},{"cell_type":"code","source":["from torchvision.io import read_image\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","img = read_image(\"example_images/bike_in_wild.jpg\")\n","\n","# Step 1: Initialize model with the best available weights\n","# weights = ResNet50_Weights.DEFAULT\n","weights = ResNet50_Weights.IMAGENET1K_V2\n","model = resnet50(weights=weights)\n","model.eval()\n","\n","# Step 2: Initialize the inference transforms\n","preprocess = weights.transforms()\n","\n","# Step 3: Apply inference preprocessing transforms\n","batch = preprocess(img).unsqueeze(0)\n","\n","# Step 4: Use the model and print the predicted category\n","prediction = model(batch).squeeze(0).softmax(0)\n","class_id = prediction.argmax().item()\n","score = prediction[class_id].item()\n","category_name = weights.meta[\"categories\"][class_id]\n","print(f\"{category_name}: {100 * score:.1f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n--4v2qExBgu","executionInfo":{"status":"ok","timestamp":1699931550711,"user_tz":300,"elapsed":1079,"user":{"displayName":"Ruihan Xu (Multy)","userId":"12479151467308550886"}},"outputId":"814d4722-b57a-4419-b6c3-52cbeab9d33f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["mountain bike: 17.7%\n"]}]},{"cell_type":"code","source":["import torch\n","from torchvision.models.feature_extraction import create_feature_extractor\n","\n","\n","x = torch.rand(1, 3, 224, 224)\n","\n","\n","return_nodes = {\n","    \"layer4.2.relu_2\": \"layer4\"\n","}\n","feature_model = create_feature_extractor(model, return_nodes=return_nodes)\n","intermediate_outputs = feature_model(x)\n","\n","print(intermediate_outputs['layer4'].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJiTnDMQxloF","executionInfo":{"status":"ok","timestamp":1699931526295,"user_tz":300,"elapsed":580,"user":{"displayName":"Ruihan Xu (Multy)","userId":"12479151467308550886"}},"outputId":"ff4435e0-ee67-4d35-ff8b-fa0f0b44d65c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n","  torch.has_cuda,\n","/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n","  torch.has_cudnn,\n","/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n","  torch.has_mps,\n","/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n","  torch.has_mkldnn,\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2048, 7, 7])\n"]}]},{"cell_type":"code","source":["from torchvision.models.feature_extraction import get_graph_node_names\n","train_nodes, eval_nodes = get_graph_node_names(resnet50())\n","for layer in train_nodes:\n","  print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQBJky_c1xWF","executionInfo":{"status":"ok","timestamp":1699932279145,"user_tz":300,"elapsed":734,"user":{"displayName":"Ruihan Xu (Multy)","userId":"12479151467308550886"}},"outputId":"2bec19e6-69c4-4911-9b1b-2dca736ebd9b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["x\n","conv1\n","bn1\n","relu\n","maxpool\n","layer1.0.conv1\n","layer1.0.bn1\n","layer1.0.relu\n","layer1.0.conv2\n","layer1.0.bn2\n","layer1.0.relu_1\n","layer1.0.conv3\n","layer1.0.bn3\n","layer1.0.downsample.0\n","layer1.0.downsample.1\n","layer1.0.add\n","layer1.0.relu_2\n","layer1.1.conv1\n","layer1.1.bn1\n","layer1.1.relu\n","layer1.1.conv2\n","layer1.1.bn2\n","layer1.1.relu_1\n","layer1.1.conv3\n","layer1.1.bn3\n","layer1.1.add\n","layer1.1.relu_2\n","layer1.2.conv1\n","layer1.2.bn1\n","layer1.2.relu\n","layer1.2.conv2\n","layer1.2.bn2\n","layer1.2.relu_1\n","layer1.2.conv3\n","layer1.2.bn3\n","layer1.2.add\n","layer1.2.relu_2\n","layer2.0.conv1\n","layer2.0.bn1\n","layer2.0.relu\n","layer2.0.conv2\n","layer2.0.bn2\n","layer2.0.relu_1\n","layer2.0.conv3\n","layer2.0.bn3\n","layer2.0.downsample.0\n","layer2.0.downsample.1\n","layer2.0.add\n","layer2.0.relu_2\n","layer2.1.conv1\n","layer2.1.bn1\n","layer2.1.relu\n","layer2.1.conv2\n","layer2.1.bn2\n","layer2.1.relu_1\n","layer2.1.conv3\n","layer2.1.bn3\n","layer2.1.add\n","layer2.1.relu_2\n","layer2.2.conv1\n","layer2.2.bn1\n","layer2.2.relu\n","layer2.2.conv2\n","layer2.2.bn2\n","layer2.2.relu_1\n","layer2.2.conv3\n","layer2.2.bn3\n","layer2.2.add\n","layer2.2.relu_2\n","layer2.3.conv1\n","layer2.3.bn1\n","layer2.3.relu\n","layer2.3.conv2\n","layer2.3.bn2\n","layer2.3.relu_1\n","layer2.3.conv3\n","layer2.3.bn3\n","layer2.3.add\n","layer2.3.relu_2\n","layer3.0.conv1\n","layer3.0.bn1\n","layer3.0.relu\n","layer3.0.conv2\n","layer3.0.bn2\n","layer3.0.relu_1\n","layer3.0.conv3\n","layer3.0.bn3\n","layer3.0.downsample.0\n","layer3.0.downsample.1\n","layer3.0.add\n","layer3.0.relu_2\n","layer3.1.conv1\n","layer3.1.bn1\n","layer3.1.relu\n","layer3.1.conv2\n","layer3.1.bn2\n","layer3.1.relu_1\n","layer3.1.conv3\n","layer3.1.bn3\n","layer3.1.add\n","layer3.1.relu_2\n","layer3.2.conv1\n","layer3.2.bn1\n","layer3.2.relu\n","layer3.2.conv2\n","layer3.2.bn2\n","layer3.2.relu_1\n","layer3.2.conv3\n","layer3.2.bn3\n","layer3.2.add\n","layer3.2.relu_2\n","layer3.3.conv1\n","layer3.3.bn1\n","layer3.3.relu\n","layer3.3.conv2\n","layer3.3.bn2\n","layer3.3.relu_1\n","layer3.3.conv3\n","layer3.3.bn3\n","layer3.3.add\n","layer3.3.relu_2\n","layer3.4.conv1\n","layer3.4.bn1\n","layer3.4.relu\n","layer3.4.conv2\n","layer3.4.bn2\n","layer3.4.relu_1\n","layer3.4.conv3\n","layer3.4.bn3\n","layer3.4.add\n","layer3.4.relu_2\n","layer3.5.conv1\n","layer3.5.bn1\n","layer3.5.relu\n","layer3.5.conv2\n","layer3.5.bn2\n","layer3.5.relu_1\n","layer3.5.conv3\n","layer3.5.bn3\n","layer3.5.add\n","layer3.5.relu_2\n","layer4.0.conv1\n","layer4.0.bn1\n","layer4.0.relu\n","layer4.0.conv2\n","layer4.0.bn2\n","layer4.0.relu_1\n","layer4.0.conv3\n","layer4.0.bn3\n","layer4.0.downsample.0\n","layer4.0.downsample.1\n","layer4.0.add\n","layer4.0.relu_2\n","layer4.1.conv1\n","layer4.1.bn1\n","layer4.1.relu\n","layer4.1.conv2\n","layer4.1.bn2\n","layer4.1.relu_1\n","layer4.1.conv3\n","layer4.1.bn3\n","layer4.1.add\n","layer4.1.relu_2\n","layer4.2.conv1\n","layer4.2.bn1\n","layer4.2.relu\n","layer4.2.conv2\n","layer4.2.bn2\n","layer4.2.relu_1\n","layer4.2.conv3\n","layer4.2.bn3\n","layer4.2.add\n","layer4.2.relu_2\n","avgpool\n","flatten\n","fc\n"]}]},{"cell_type":"code","source":["from torchsummary import summary\n","summary(model, (3, 244, 244))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IuXAqbTr0I_g","executionInfo":{"status":"ok","timestamp":1699931739790,"user_tz":300,"elapsed":510,"user":{"displayName":"Ruihan Xu (Multy)","userId":"12479151467308550886"}},"outputId":"e4e21c3c-1b27-4251-cf67-01a54fce2d57"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 122, 122]           9,408\n","       BatchNorm2d-2         [-1, 64, 122, 122]             128\n","              ReLU-3         [-1, 64, 122, 122]               0\n","         MaxPool2d-4           [-1, 64, 61, 61]               0\n","            Conv2d-5           [-1, 64, 61, 61]           4,096\n","       BatchNorm2d-6           [-1, 64, 61, 61]             128\n","              ReLU-7           [-1, 64, 61, 61]               0\n","            Conv2d-8           [-1, 64, 61, 61]          36,864\n","       BatchNorm2d-9           [-1, 64, 61, 61]             128\n","             ReLU-10           [-1, 64, 61, 61]               0\n","           Conv2d-11          [-1, 256, 61, 61]          16,384\n","      BatchNorm2d-12          [-1, 256, 61, 61]             512\n","           Conv2d-13          [-1, 256, 61, 61]          16,384\n","      BatchNorm2d-14          [-1, 256, 61, 61]             512\n","             ReLU-15          [-1, 256, 61, 61]               0\n","       Bottleneck-16          [-1, 256, 61, 61]               0\n","           Conv2d-17           [-1, 64, 61, 61]          16,384\n","      BatchNorm2d-18           [-1, 64, 61, 61]             128\n","             ReLU-19           [-1, 64, 61, 61]               0\n","           Conv2d-20           [-1, 64, 61, 61]          36,864\n","      BatchNorm2d-21           [-1, 64, 61, 61]             128\n","             ReLU-22           [-1, 64, 61, 61]               0\n","           Conv2d-23          [-1, 256, 61, 61]          16,384\n","      BatchNorm2d-24          [-1, 256, 61, 61]             512\n","             ReLU-25          [-1, 256, 61, 61]               0\n","       Bottleneck-26          [-1, 256, 61, 61]               0\n","           Conv2d-27           [-1, 64, 61, 61]          16,384\n","      BatchNorm2d-28           [-1, 64, 61, 61]             128\n","             ReLU-29           [-1, 64, 61, 61]               0\n","           Conv2d-30           [-1, 64, 61, 61]          36,864\n","      BatchNorm2d-31           [-1, 64, 61, 61]             128\n","             ReLU-32           [-1, 64, 61, 61]               0\n","           Conv2d-33          [-1, 256, 61, 61]          16,384\n","      BatchNorm2d-34          [-1, 256, 61, 61]             512\n","             ReLU-35          [-1, 256, 61, 61]               0\n","       Bottleneck-36          [-1, 256, 61, 61]               0\n","           Conv2d-37          [-1, 128, 61, 61]          32,768\n","      BatchNorm2d-38          [-1, 128, 61, 61]             256\n","             ReLU-39          [-1, 128, 61, 61]               0\n","           Conv2d-40          [-1, 128, 31, 31]         147,456\n","      BatchNorm2d-41          [-1, 128, 31, 31]             256\n","             ReLU-42          [-1, 128, 31, 31]               0\n","           Conv2d-43          [-1, 512, 31, 31]          65,536\n","      BatchNorm2d-44          [-1, 512, 31, 31]           1,024\n","           Conv2d-45          [-1, 512, 31, 31]         131,072\n","      BatchNorm2d-46          [-1, 512, 31, 31]           1,024\n","             ReLU-47          [-1, 512, 31, 31]               0\n","       Bottleneck-48          [-1, 512, 31, 31]               0\n","           Conv2d-49          [-1, 128, 31, 31]          65,536\n","      BatchNorm2d-50          [-1, 128, 31, 31]             256\n","             ReLU-51          [-1, 128, 31, 31]               0\n","           Conv2d-52          [-1, 128, 31, 31]         147,456\n","      BatchNorm2d-53          [-1, 128, 31, 31]             256\n","             ReLU-54          [-1, 128, 31, 31]               0\n","           Conv2d-55          [-1, 512, 31, 31]          65,536\n","      BatchNorm2d-56          [-1, 512, 31, 31]           1,024\n","             ReLU-57          [-1, 512, 31, 31]               0\n","       Bottleneck-58          [-1, 512, 31, 31]               0\n","           Conv2d-59          [-1, 128, 31, 31]          65,536\n","      BatchNorm2d-60          [-1, 128, 31, 31]             256\n","             ReLU-61          [-1, 128, 31, 31]               0\n","           Conv2d-62          [-1, 128, 31, 31]         147,456\n","      BatchNorm2d-63          [-1, 128, 31, 31]             256\n","             ReLU-64          [-1, 128, 31, 31]               0\n","           Conv2d-65          [-1, 512, 31, 31]          65,536\n","      BatchNorm2d-66          [-1, 512, 31, 31]           1,024\n","             ReLU-67          [-1, 512, 31, 31]               0\n","       Bottleneck-68          [-1, 512, 31, 31]               0\n","           Conv2d-69          [-1, 128, 31, 31]          65,536\n","      BatchNorm2d-70          [-1, 128, 31, 31]             256\n","             ReLU-71          [-1, 128, 31, 31]               0\n","           Conv2d-72          [-1, 128, 31, 31]         147,456\n","      BatchNorm2d-73          [-1, 128, 31, 31]             256\n","             ReLU-74          [-1, 128, 31, 31]               0\n","           Conv2d-75          [-1, 512, 31, 31]          65,536\n","      BatchNorm2d-76          [-1, 512, 31, 31]           1,024\n","             ReLU-77          [-1, 512, 31, 31]               0\n","       Bottleneck-78          [-1, 512, 31, 31]               0\n","           Conv2d-79          [-1, 256, 31, 31]         131,072\n","      BatchNorm2d-80          [-1, 256, 31, 31]             512\n","             ReLU-81          [-1, 256, 31, 31]               0\n","           Conv2d-82          [-1, 256, 16, 16]         589,824\n","      BatchNorm2d-83          [-1, 256, 16, 16]             512\n","             ReLU-84          [-1, 256, 16, 16]               0\n","           Conv2d-85         [-1, 1024, 16, 16]         262,144\n","      BatchNorm2d-86         [-1, 1024, 16, 16]           2,048\n","           Conv2d-87         [-1, 1024, 16, 16]         524,288\n","      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n","             ReLU-89         [-1, 1024, 16, 16]               0\n","       Bottleneck-90         [-1, 1024, 16, 16]               0\n","           Conv2d-91          [-1, 256, 16, 16]         262,144\n","      BatchNorm2d-92          [-1, 256, 16, 16]             512\n","             ReLU-93          [-1, 256, 16, 16]               0\n","           Conv2d-94          [-1, 256, 16, 16]         589,824\n","      BatchNorm2d-95          [-1, 256, 16, 16]             512\n","             ReLU-96          [-1, 256, 16, 16]               0\n","           Conv2d-97         [-1, 1024, 16, 16]         262,144\n","      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n","             ReLU-99         [-1, 1024, 16, 16]               0\n","      Bottleneck-100         [-1, 1024, 16, 16]               0\n","          Conv2d-101          [-1, 256, 16, 16]         262,144\n","     BatchNorm2d-102          [-1, 256, 16, 16]             512\n","            ReLU-103          [-1, 256, 16, 16]               0\n","          Conv2d-104          [-1, 256, 16, 16]         589,824\n","     BatchNorm2d-105          [-1, 256, 16, 16]             512\n","            ReLU-106          [-1, 256, 16, 16]               0\n","          Conv2d-107         [-1, 1024, 16, 16]         262,144\n","     BatchNorm2d-108         [-1, 1024, 16, 16]           2,048\n","            ReLU-109         [-1, 1024, 16, 16]               0\n","      Bottleneck-110         [-1, 1024, 16, 16]               0\n","          Conv2d-111          [-1, 256, 16, 16]         262,144\n","     BatchNorm2d-112          [-1, 256, 16, 16]             512\n","            ReLU-113          [-1, 256, 16, 16]               0\n","          Conv2d-114          [-1, 256, 16, 16]         589,824\n","     BatchNorm2d-115          [-1, 256, 16, 16]             512\n","            ReLU-116          [-1, 256, 16, 16]               0\n","          Conv2d-117         [-1, 1024, 16, 16]         262,144\n","     BatchNorm2d-118         [-1, 1024, 16, 16]           2,048\n","            ReLU-119         [-1, 1024, 16, 16]               0\n","      Bottleneck-120         [-1, 1024, 16, 16]               0\n","          Conv2d-121          [-1, 256, 16, 16]         262,144\n","     BatchNorm2d-122          [-1, 256, 16, 16]             512\n","            ReLU-123          [-1, 256, 16, 16]               0\n","          Conv2d-124          [-1, 256, 16, 16]         589,824\n","     BatchNorm2d-125          [-1, 256, 16, 16]             512\n","            ReLU-126          [-1, 256, 16, 16]               0\n","          Conv2d-127         [-1, 1024, 16, 16]         262,144\n","     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n","            ReLU-129         [-1, 1024, 16, 16]               0\n","      Bottleneck-130         [-1, 1024, 16, 16]               0\n","          Conv2d-131          [-1, 256, 16, 16]         262,144\n","     BatchNorm2d-132          [-1, 256, 16, 16]             512\n","            ReLU-133          [-1, 256, 16, 16]               0\n","          Conv2d-134          [-1, 256, 16, 16]         589,824\n","     BatchNorm2d-135          [-1, 256, 16, 16]             512\n","            ReLU-136          [-1, 256, 16, 16]               0\n","          Conv2d-137         [-1, 1024, 16, 16]         262,144\n","     BatchNorm2d-138         [-1, 1024, 16, 16]           2,048\n","            ReLU-139         [-1, 1024, 16, 16]               0\n","      Bottleneck-140         [-1, 1024, 16, 16]               0\n","          Conv2d-141          [-1, 512, 16, 16]         524,288\n","     BatchNorm2d-142          [-1, 512, 16, 16]           1,024\n","            ReLU-143          [-1, 512, 16, 16]               0\n","          Conv2d-144            [-1, 512, 8, 8]       2,359,296\n","     BatchNorm2d-145            [-1, 512, 8, 8]           1,024\n","            ReLU-146            [-1, 512, 8, 8]               0\n","          Conv2d-147           [-1, 2048, 8, 8]       1,048,576\n","     BatchNorm2d-148           [-1, 2048, 8, 8]           4,096\n","          Conv2d-149           [-1, 2048, 8, 8]       2,097,152\n","     BatchNorm2d-150           [-1, 2048, 8, 8]           4,096\n","            ReLU-151           [-1, 2048, 8, 8]               0\n","      Bottleneck-152           [-1, 2048, 8, 8]               0\n","          Conv2d-153            [-1, 512, 8, 8]       1,048,576\n","     BatchNorm2d-154            [-1, 512, 8, 8]           1,024\n","            ReLU-155            [-1, 512, 8, 8]               0\n","          Conv2d-156            [-1, 512, 8, 8]       2,359,296\n","     BatchNorm2d-157            [-1, 512, 8, 8]           1,024\n","            ReLU-158            [-1, 512, 8, 8]               0\n","          Conv2d-159           [-1, 2048, 8, 8]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 8, 8]           4,096\n","            ReLU-161           [-1, 2048, 8, 8]               0\n","      Bottleneck-162           [-1, 2048, 8, 8]               0\n","          Conv2d-163            [-1, 512, 8, 8]       1,048,576\n","     BatchNorm2d-164            [-1, 512, 8, 8]           1,024\n","            ReLU-165            [-1, 512, 8, 8]               0\n","          Conv2d-166            [-1, 512, 8, 8]       2,359,296\n","     BatchNorm2d-167            [-1, 512, 8, 8]           1,024\n","            ReLU-168            [-1, 512, 8, 8]               0\n","          Conv2d-169           [-1, 2048, 8, 8]       1,048,576\n","     BatchNorm2d-170           [-1, 2048, 8, 8]           4,096\n","            ReLU-171           [-1, 2048, 8, 8]               0\n","      Bottleneck-172           [-1, 2048, 8, 8]               0\n","AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n","          Linear-174                 [-1, 1000]       2,049,000\n","================================================================\n","Total params: 25,557,032\n","Trainable params: 25,557,032\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.68\n","Forward/backward pass size (MB): 351.15\n","Params size (MB): 97.49\n","Estimated Total Size (MB): 449.33\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XRQnYszt2yEb"},"execution_count":null,"outputs":[]}]}